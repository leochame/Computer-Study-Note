{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOw+Kwpn2wvycurBhoXewTK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leochame/Computer-Study-Note/blob/main/llm/llms-from-srcatch/ch03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 此内容为代码格式\n",
        "```\n",
        "\n",
        "# 前言\n",
        "在本章我们研究最重要的注意力机制。\n",
        "我们将要实现的不同注意力机制。我们将从一个简化版本的自注意力机制开始，然后逐步加入可训练的权重。因果注意力机制在自注意力的基础上增加了额外掩码，使得大语言模型可以一次生成一个单词。最后，多头注意力将注意力机制划分成多个头，从而使模型能够并行捕获输入数据的各种特征"
      ],
      "metadata": {
        "id": "uf4CuCbc95q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "注意力机制的核心思想是对文本进行任何操作，在生成文本的地方，我们随时可以查看整个输入。因此"
      ],
      "metadata": {
        "id": "KvItoChhC8WV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3 简单注意力机制"
      ],
      "metadata": {
        "id": "7b2NoW4hNhvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "自注意力机制（Self-Attention）是什么？\n",
        "自”指的是自注意力机制是作用在单个输入序列上的，而不是两个不同的序列。也就是说，在计算一个序列的表示时，它会关注序列内部不同位置之间的关系，而不是序列之间的关系。例如，在处理句子时，它会评估句子中不同单词之间的依赖关系，而不是输入和输出之间的依赖。例子：\n",
        "\n",
        "* 在句子“猫追逐着小狗”的处理中，自注意力机制会计算“猫”和“追逐”之间、“追逐”和“小狗”之间的关系，以及它们如何共同影响句子的理解。\n",
        "\n",
        "传统的注意力机制通常用于序列到序列（seq2seq）模型中，比如在机器翻译中，输入序列和输出序列之间存在关联。注意力机制会在这两个序列间寻找关联，而不是单纯在输入序列内部进行计算。\n",
        "* 比如，机器翻译时，输入句子（如英语）会与输出句子（如中文）之间的某些位置进行对齐，注意力机制会根据这一对齐关系分配不同的权重"
      ],
      "metadata": {
        "id": "6foRsf8F2ak_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "自注意力机制的关键在于**计算注意力权重**，这些权重决定了序列中不同部分的重要性。这些权重并不是固定的，而是可以通过训练数据来学习的，这就是“带可训练权重”的含义。通过优化这些权重，模型能够更好地捕捉序列内部的依赖关系。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mRjGKwM82vN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们首先实现一个不包含任何可训练权重的简化的自注意力机制变体\n",
        "\n"
      ],
      "metadata": {
        "id": "hUEr4T6A26im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "考虑输入文本“Your journey starts with one step.”在这种情况下，文本序列中的每个元素都对应一个 d\n",
        "维的嵌入向量，该向量代表了一个特定的词元，比如“Your”​。"
      ],
      "metadata": {
        "id": "MtezQ43c35Iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在自注意力机制中，我们的目标是为输入序列中的每个元素  X^i 计算上下文向量 Z^i。上下文向量(context vector)可以被理解为一种包含了序列中所有元素信息的嵌入向量。"
      ],
      "metadata": {
        "id": "fT0jGKN14Fvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们选择较小的嵌入维度进行说明。"
      ],
      "metadata": {
        "id": "QCD6x7gT4tdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "8HTd6eSlEI8v"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "如何计算查询词元与每个输入词元之间的中间注意力分数呢？\n",
        "\n",
        "我们通过计算查询词元 X^2 与其他所有输入词元的点积来确定这些分数\n",
        "\n",
        "我们之后会采用torch..dot来计算点积。我们先写一个点积含义的例子，提供给你参考。"
      ],
      "metadata": {
        "id": "jnq4jqx05mUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "  attn_scores_2[i] = torch.dot(x_i,query)\n",
        "print(attn_scores_2)\n",
        "\n",
        "\n",
        "res = 0.\n",
        "for idx, element in enumerate(inputs[0]):\n",
        "    res += inputs[0][idx] * query[idx]\n",
        "print(res)\n",
        "print(torch.dot(inputs[0], query))"
      ],
      "metadata": {
        "id": "lKxhFWA-25w-",
        "outputId": "e5b1aa45-3791-4eda-ed92-182af93b2527",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
            "tensor(0.9544)\n",
            "tensor(0.9544)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "点积不仅被视为一种将两个向量转化为标量值的数学工具，而且也是度量相似度的一种方式，因为它可以量化两个向量之间的对齐程度：点积越大，向量之间的对齐程度或相似度就越高。在自注意机制中，点积决定了序列中每个元素对其他元素的关注程度：点积越大，两个元素之间的相似度和注意力分数就越高。   \n",
        "\n",
        "在下一步中，我们将对先前计算的每个注意力分数进行归一化处理。归一化的主要目的是获得总和为1的注意力权重。"
      ],
      "metadata": {
        "id": "HBK3AZyh6RWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "print(\"Attention weights:\", attn_weights_2_tmp)\n",
        "print(\"Sum:\", attn_weights_2_tmp.sum())"
      ],
      "metadata": {
        "id": "Am8PtS1J_StW",
        "outputId": "9c5093cb-edff-4067-d13c-3af109190b08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum: tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在实际应用中，使用softmax函数进行归一化更为常见，而且是一种更可取的做法。这种方法更好地处理了极值，并在训练期间提供了更有利的梯度特性。\n",
        "\n",
        "\n",
        "另外，softmax函数可以保证注意力权重总是正值，这使得输出可以被解释为概率或相对重要性，其中权重越高表示重要程度越高。"
      ],
      "metadata": {
        "id": "ZVNrT45q7E19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "print(\"Attention weights:\", attn_weights_2)\n",
        "print(\"Sum:\", attn_weights_2.sum())"
      ],
      "metadata": {
        "id": "qAjioqQs7Fn5",
        "outputId": "73539ec4-e8b8-4fe6-971e-ce7a34ecd110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过将每个输入词元的嵌入向量与它对应的注意力权重相乘，再将这些结果加起来，得到该词元的上下文向量。  \n",
        "> 这个过程本质上是通过加权求和来聚合序列中各个元素的信息，从而为每个词元生成一个综合了其他元素信息的表示。"
      ],
      "metadata": {
        "id": "tpRu9rya8tFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i,x_i in enumerate(inputs):\n",
        "  context_vec_2 += attn_weights_2[i]*x_i\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "id": "cQCxLpgi8uUx",
        "outputId": "629ba331-e04c-4fe5-fb53-00ee381f403d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 计算所有输入词元的注意力权重"
      ],
      "metadata": {
        "id": "w4pIkCbY9ttn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "到目前为止，我们已经计算了输入2的注意力权重和上下文向量接下来，我们将扩展这个计算过程，以计算所有输入的注意力权重和上下文向量。"
      ],
      "metadata": {
        "id": "m4QTBCbz-N0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.empty(6, 6)\n",
        "for i, x_i in enumerate(inputs):\n",
        "    for j, x_j in enumerate(inputs):\n",
        "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "id": "tIkbAmok-P5o",
        "outputId": "ee6809bc-d25d-47a9-edcc-e0efbc5aa7ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在计算前面的注意力分数张量时，我们使用了Python中的for循环。然而，for循环通常较慢，因此可以使用矩阵乘法来得到相同的结果："
      ],
      "metadata": {
        "id": "zLmGYoDJ-fSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "id": "a7a35gUM-dNj",
        "outputId": "afebee56-2b9b-4c6d-afa4-3158bbbc648c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 对每一行进行归一化，以确保每一行中的值总和为1\n",
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "id": "2YMs5xSO-ia0",
        "outputId": "fc8f6319-c27e-4d10-ea52-b6dc3301fc83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 验证一下每一行的总和是否确实为1：\n",
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
        "print(\"Row 2 sum:\", row_2_sum)\n",
        "print(\"All row sums:\", attn_weights.sum(dim=-1))"
      ],
      "metadata": {
        "id": "5n4sKSe4-vJt",
        "outputId": "de74f9c3-33cb-4b4c-dce1-6f4abe9205f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2 sum: 1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "最后一步，我们用这些注意力权重通过矩阵乘法计算出所有**上下文向量**："
      ],
      "metadata": {
        "id": "-U3-9eLj-4c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ],
      "metadata": {
        "id": "Z3wJwlKt-8As",
        "outputId": "8561634c-1650-4941-e327-44fcff912f10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在，简单自注意力机制的代码已经完成了。\n",
        "接下来，我们将添加可训练的权重，使大语言模型能够从数据中学习，并提升其在特定任务上的性能。"
      ],
      "metadata": {
        "id": "bI6iZuSH_IVk"
      }
    }
  ]
}