{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWI0I4kRm/2sWQBul4HqAT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leochame/Computer-Study-Note/blob/main/llm/llms-from-srcatch/ch04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#\n",
        "注意力机制是LLM的核心，那么接下来我们将组装剩下的组件"
      ],
      "metadata": {
        "id": "HqVHDlh8hIhM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFMoLIZMhAXr",
        "outputId": "d91f5155-976a-4693-dba0-ce8911102682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matplotlib version: 3.10.0\n",
            "torch version: 2.6.0+cu124\n",
            "tiktoken version: 0.9.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "import matplotlib\n",
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
        "#加载并确认版本"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "#初始化定义需要的各种超参数"
      ],
      "metadata": {
        "id": "tBhiGcBKTVXi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上面是配置，"
      ],
      "metadata": {
        "id": "vbzDWMttTd6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/03.webp\" width=\"500px\">"
      ],
      "metadata": {
        "id": "OcCAPJWggznY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们编写GPT架构的步骤是：首先从GPT主干入手，创建一个占位符架构；然后实现各个核心组件；最后将它们组装成Transformer块，形成完整的GPT架构。\n",
        "\n",
        "上方图中的编号框展示了我们处理编写最终GPT架构所需的各个概念的顺序。我们将从第(1)步开始，创建一个名为DummyGPTModel的占位符GPT主干部分"
      ],
      "metadata": {
        "id": "JOPfSgAOhW8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        # 词嵌入层，将输入索引转换为词向量，词表大小由字典大小和特征维度决定。\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        # 位置信息嵌入层，基于文本长度和特征维度生成位置信息。\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "        # Dropout 层，用于随机丢弃一部分嵌入信息以减少过拟合。\n",
        "\n",
        "        ### 使用多个 Transformer 块（占位符）\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "        # Transformer 模块的堆叠，模型核心部分。\n",
        "\n",
        "        ### 使用归一化层（占位符）\n",
        "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "        # 最终归一化层，用于调整特征分布。\n",
        "\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "        # 输出层，将特征映射到词表分布，最终预测输出单词。\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        # 获取批次大小和序列长度。\n",
        "\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        # 根据输入索引生成词嵌入。\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        # 生成对应的位置信息嵌入。\n",
        "\n",
        "        x = tok_embeds + pos_embeds\n",
        "        # 将词嵌入和位置信息嵌入相加。\n",
        "        x = self.drop_emb(x)\n",
        "        # 应用 Dropout 随机丢弃部分信息。\n",
        "        x = self.trf_blocks(x)\n",
        "        # 通过多个 Transformer 块处理特征。\n",
        "        x = self.final_norm(x)\n",
        "        # 应用最终的归一化层。\n",
        "        logits = self.out_head(x)\n",
        "        # 将隐藏状态映射到词表分布，生成预测结果。\n",
        "        return logits\n",
        "\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "    # Transformer 块的占位类。\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # 占位，实际模型应实现注意力机制和前馈网络。\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 此块不执行任何操作，仅返回输入。\n",
        "        return x\n",
        "\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "    # 归一化层的占位类。\n",
        "    def __init__(self, normalized_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        # 参数用于模拟 LayerNorm 的接口。\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 此层不执行任何操作，仅返回输入。\n",
        "        return x"
      ],
      "metadata": {
        "id": "UD12ODetTeME"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上面的类是一个简化版的类 GPT 模型。包括词元和位置潜入，dropout、一系列 Transformer 块，最终层归一化。\n",
        "\n",
        "\n",
        "forward方法描述了数据在模型中的处理流程：它首先计算输入索引的词元和位置嵌入，然后应用dropout，接着通过Transformer块处理数据，再应用归一化，最后使用线性输出层生成logits。"
      ],
      "metadata": {
        "id": "TzQcAm1Ch-Vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们接下来介绍一下 GPT 架构的输入和输出，现在我们先宏观概述数据在GPT模型中的流入和流出过程："
      ],
      "metadata": {
        "id": "Lanjoj3Ri0Wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/04.webp?123\" width=\"500px\">"
      ],
      "metadata": {
        "id": "N-wykyS1jA8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "batch = []\n",
        "\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "IzP3KE1LTdjQ",
        "outputId": "b833aa2a-5fd0-433f-ba11-de6ee8cda596",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们看到输出张量包含两行，对应于两个文本样本。每个文本样本由 4 个词元组成。每个词元是一个 50257 dimensions 的向量。"
      ],
      "metadata": {
        "id": "aEPRGGtVjReF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "logits = model(batch)\n",
        "print(\"Output shape:\", logits.shape)\n",
        "print(logits)"
      ],
      "metadata": {
        "id": "LOMQ_8JmKyBl",
        "outputId": "bd53dfcc-783d-4864-ab85-c701ccd95922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
            "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
            "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
            "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
            "\n",
            "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
            "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
            "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
            "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 使用层归一化进行归一化激活\n",
        "\n",
        "现在我们将实现层归一化。层归一化的主要思想是调整神经网络层的激活（输出）​，使其均值为0且方差（单位方差）为1。\n",
        "这种方法能够稳定训练过程，并加速权重的高效收敛。\n",
        "在 Transformer 块中，层归一化会在多头注意力模块的前后应用（我们将在后续实现），并在最终输出层之前再次应用。"
      ],
      "metadata": {
        "id": "fdRELX3SewSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们实现了一个具有5个输入和6个输出的神经网络层，并将其应用于两个输入示例："
      ],
      "metadata": {
        "id": "c3cnUIuBoNfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# create 2 training examples with 5 dimensions (features) each\n",
        "batch_example = torch.randn(2, 5)\n",
        "\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "out = layer(batch_example)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "6jvFyvLbnC_T",
        "outputId": "dc2c23a4-7ac7-4911-8373-5c28df92559d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
            "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
            "       grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "接下来我们执行归一化操作"
      ],
      "metadata": {
        "id": "ePgVAjejoHW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = out.mean(dim=-1, keepdim=True)\n",
        "var = out.var(dim=-1, keepdim=True)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "id": "vtb9_PzroDHR",
        "outputId": "2cb52b45-acb1-4a18-ee5f-dc183ef57fc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[0.1324],\n",
            "        [0.2170]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[0.0231],\n",
            "        [0.0398]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_norm = (out - mean) / torch.sqrt(var)\n",
        "#执行归一化操作\n",
        "print(\"Normalized layer outputs:\\n\", out_norm)\n",
        "\n",
        "mean = out_norm.mean(dim=-1, keepdim=True)\n",
        "var = out_norm.var(dim=-1, keepdim=True)\n",
        "\n",
        "# 关闭科学计数法 去打印张量值\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "id": "Iz5hg7P0n2Ih",
        "outputId": "56c9a113-d67d-4f01-8c94-d38c73e570e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized layer outputs:\n",
            " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
            "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "Mean:\n",
            " tensor([[    -0.0000],\n",
            "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们将这个过程封装成一个PyTorch模块，以便后续使用"
      ],
      "metadata": {
        "id": "Mq-KkiJdot9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 定义一个神经网络层，专门用于层归一化。\n",
        "# 继承 PyTorch 的基础类 nn.Module 是标准写法，能让这个类自动管理所有可学习的参数。\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    层归一化 (Layer Normalization) 的代码实现。\n",
        "    \"\"\"\n",
        "\n",
        "    # 类的初始化函数。在创建这个层的实例时，会运行一次，用于定义层内部需要的变量和参数。\n",
        "    # emb_dim: 传入的特征向量维度。例如，如果每个词向量是768维，emb_dim就是768。\n",
        "    def __init__(self, emb_dim):\n",
        "        # 调用父类的初始化函数，必须执行。\n",
        "        super().__init__()\n",
        "\n",
        "        # 定义一个很小的数值 epsilon。\n",
        "        # 它的唯一作用是防止在后续计算中，因方差为零而出现除以零的程序错误。\n",
        "        self.eps = 1e-5\n",
        "\n",
        "        # 定义一个名为 'scale' 的可学习参数。\n",
        "        # nn.Parameter() 会将这个张量注册为模型的参数，意味着它的值会在训练过程中被自动学习和更新。\n",
        "        # 它的功能是对归一化后的数据进行缩放。初始值为1，表示不做任何缩放。\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "\n",
        "        # 定义一个名为 'shift' 的可学习参数。\n",
        "        # 同样，它也是一个模型在训练中会自动学习和更新的参数。\n",
        "        # 它的功能是对归一化后的数据进行平移。初始值为0，表示不做任何平移。\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    # 定义层的前向传播函数。当数据输入到这个层时，这里的代码会被执行。\n",
        "    # x: 输入的数据张量，形状通常是 [批次大小, 序列长度, 特征维度]。\n",
        "    def forward(self, x):\n",
        "        # --- 步骤 1: 计算均值 ---\n",
        "        # `dim=-1` 指示沿着最后一个维度（即特征维度）计算。\n",
        "        # `keepdim=True` 保持结果的维度结构，方便后续的向量化计算。\n",
        "        # 这一步的目的是得到每个数据样本在特征维度上的平均值。\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "\n",
        "        # --- 步骤 2: 计算方差 ---\n",
        "        # 同样，沿着最后一个维度计算方差。\n",
        "        # 这一步的目的是得到每个数据样本在特征维度上的数值分散程度。\n",
        "        # `unbiased=False` 表示使用总体方差进行计算。\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "\n",
        "        # --- 步骤 3: 标准化数据 ---\n",
        "        # 将每个数据点减去其所在样本的均值，再除以其标准差（方差的平方根）。\n",
        "        # `var + self.eps` 就是为了防止除零错误。\n",
        "        # 经过这步操作后，输出 `norm_x` 的均值会变为0，方差变为1。\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "\n",
        "        # --- 步骤 4: 应用学习到的缩放和平移 ---\n",
        "        # 将上一步标准化后的数据 `norm_x`，乘以模型学习到的 `scale` 参数，再加上 `shift` 参数。\n",
        "        # 这一步给了模型一个“机会”，可以根据任务的需要，将数据调整到它认为最理想的分布状态，\n",
        "        # 而不只是强制固定在均值为0、方差为1。\n",
        "        return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "ZTWzPGMqn689"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 除了通过减去均值并除以方差来执行归一化操作外，我们还引入了两个可训练参数：`scale`（缩放参数）和 `shift`（平移参数）。\n",
        "- 初始时，`scale` 值为 1，`shift` 值为 0，不会对结果产生影响；但在训练过程中，LLM 会自动调整这两个参数，以提升模型在任务中的表现。\n",
        "- 这种设计使模型能够学习到最适合其数据的缩放和平移方式。\n",
        "- 此外，在计算方差的平方根时，我们会添加一个较小的值（`eps`），以避免方差为 0 时出现除以 0 的错误。"
      ],
      "metadata": {
        "id": "tbnJYi0WqQDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln = ln(batch_example)\n",
        "mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "id": "1ojI3yMAqPWc",
        "outputId": "7437a65b-c20d-4b96-b892-0993c4c20bd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[    -0.0000],\n",
            "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    }
  ]
}